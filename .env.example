# ================================================
# Clockify RAG Service Configuration (Remote-First)
# ================================================
#
# This configuration is optimized for Mac M1 Pro + corporate VPN setup.
# All LLM generation and embeddings run remotely on the corporate Ollama instance.
#
# Copy this file to .env and customize for your deployment.
# All values are examples - adjust according to your environment.

# ====== OLLAMA / LLM CONFIGURATION (Remote-First) ======

# Corporate Ollama endpoint (reachable only over VPN)
# Default: http://10.127.0.192:11434 (corporate VPN host)
# For local testing: override to http://127.0.0.1:11434
RAG_OLLAMA_URL=http://10.127.0.192:11434

# LLM generation model (primary choice, will auto-fallback if unavailable)
# Default: qwen2.5:32b (more capable, 32B parameters)
# If unavailable, falls back to RAG_CHAT_FALLBACK_MODEL
RAG_CHAT_MODEL=qwen2.5:32b

# LLM fallback model (used if primary model is unavailable)
# Default: gpt-oss:20b (lighter weight, 20B parameters)
# Smart selection ensures service resilience on VPN flakiness
RAG_CHAT_FALLBACK_MODEL=gpt-oss:20b

# Remote Ollama timeout (in seconds)
# Default: 120 seconds (VPN can be slow; balance responsiveness with reliability)
# Range: 5-600 seconds
OLLAMA_TIMEOUT=120.0

# Embedding model (always remote, no local SentenceTransformer)
# Default: nomic-embed-text:latest (768-dimensional, semantic embeddings)
RAG_EMBED_MODEL=nomic-embed-text:latest

# Optional: set to "mock" to force the deterministic offline client (useful for CI/testing)
# RAG_LLM_CLIENT=mock

# Legacy aliases (OLLAMA_URL / GEN_MODEL / EMB_MODEL) remain supported but RAG_* names are preferred.

# ====== EMBEDDING BACKEND ======
# Choose embedding backend: "local" (SentenceTransformer on Mac) or "ollama" (remote)
# Remote-first design recommends "ollama" to avoid local GPU load on M1 Pro
# Local option provided for legacy compatibility
# Warning: Switching backends requires re-indexing (different embedding dimensions)
EMB_BACKEND=ollama

# ====== RETRIEVAL PARAMETERS ======
# Default top-K candidates for retrieval (1-100)
DEFAULT_TOP_K=15

# Maximum top-K ceiling (safety cap to prevent context overflow)
# User-supplied top_k values are clamped to this maximum
MAX_TOP_K=50

# Number of snippets to pack in context (1-50)
DEFAULT_PACK_TOP=8

# Cosine similarity threshold (0.0-1.0)
DEFAULT_THRESHOLD=0.25

# Hybrid retrieval weight: alpha*BM25 + (1-alpha)*dense (0.0-1.0)
ALPHA=0.5

# Enable intent-based retrieval for improved accuracy
USE_INTENT_CLASSIFICATION=1

# ====== CONTEXT AND PERFORMANCE ======
# Context token budget (effective context is min of this and num_ctx*0.6)
CTX_BUDGET=12000

# LLM context window size
DEFAULT_NUM_CTX=32768

# Maximum tokens to generate
DEFAULT_NUM_PREDICT=512

# MMR lambda for diversity vs relevance (0.0-1.0)
MMR_LAMBDA=0.75

# ====== ANNOTATION NEAREST NEIGHBORS (ANN) ======
# ANN backend: "faiss" or "none" (for full-scan)
ANN=faiss

# FAISS parameters
ANN_NLIST=64
ANN_NPROBE=16
FAISS_CANDIDATE_MULTIPLIER=3
ANN_CANDIDATE_MIN=200

# ====== BM25 PARAMETERS (for technical documentation) ======
BM25_K1=1.2
BM25_B=0.65

# ====== TIMEOUT CONFIGURATION ======
# Embedding timeouts (connect/read in seconds)
# Used by embeddings_client.py for remote Ollama embedding calls
# EMBEDDING_CONNECT_TIMEOUT: Time to establish connection (default: 5.0s, range: 1-60s)
# EMBEDDING_READ_TIMEOUT: Time to receive embedding response (default: 60.0s, range: 5-300s)
EMBEDDING_CONNECT_TIMEOUT=5.0
EMBEDDING_READ_TIMEOUT=60.0

# Legacy aliases (still supported, but EMBEDDING_* names preferred)
# EMB_CONNECT_TIMEOUT=5.0
# EMB_READ_TIMEOUT=60.0

# Chat/LM timeouts (connect/read in seconds)
# CHAT_CONNECT_TIMEOUT: Time to establish connection to LLM (default: 3s)
# CHAT_READ_TIMEOUT: Time to receive LLM response (default: 120s)
CHAT_CONNECT_TIMEOUT=3.0
CHAT_READ_TIMEOUT=120.0

# Rerank timeout (for advanced reranking models, if used)
# RERANK_READ_TIMEOUT: Time to receive reranking response (default: 180s)
RERANK_READ_TIMEOUT=180.0

# ====== ADVANCED EMBEDDING CONFIGURATION ======
# EMB_MAX_WORKERS: Parallel embedding workers (default: 8, range: 1-64)
# Used for KB build to speed up embedding generation
EMB_MAX_WORKERS=8

# EMB_BATCH_SIZE: Texts per embedding batch (default: 32, range: 1-1000)
# Larger batches = faster but more memory intensive
EMB_BATCH_SIZE=32

# ====== API GATEKEEPING ======
# API auth: set to "api_key" and provide comma-separated API_ALLOWED_KEYS to enforce shared secret auth
API_AUTH_MODE=none
API_ALLOWED_KEYS=
API_KEY_HEADER=x-api-key

# ====== CACHING AND RATE LIMITING ======
# Query cache size
CACHE_MAXSIZE=100

# Cache TTL in seconds
CACHE_TTL=3600

# Rate limiting: max requests per window
RATE_LIMIT_REQUESTS=10

# Rate limiting window in seconds
RATE_LIMIT_WINDOW=60

# ====== BUILD AND INDEXING ======
# Build lock TTL in seconds
BUILD_LOCK_TTL_SEC=900

# Warm-up on startup
WARMUP=1
# Auto-download required NLTK corpora on startup (set to 0 for fully offline images)
NLTK_AUTO_DOWNLOAD=1

# ====== CHUNKING PARAMETERS ======
# Chunk size and overlap
CHUNK_CHARS=1600
CHUNK_OVERLAP=200

# ====== QUERY LOGGING ======
# Query log file path
RAG_LOG_FILE=rag_queries.jsonl

# Include answer text in logs (set to 0 to redact for privacy)
RAG_LOG_INCLUDE_ANSWER=1

# Answer placeholder when redacted
RAG_LOG_ANSWER_PLACEHOLDER=[REDACTED]

# Include chunk text in logs (set to 0 for security/privacy)
RAG_LOG_INCLUDE_CHUNKS=0

# Enable strict citation validation
RAG_STRICT_CITATIONS=0

# ====== PROXY CONFIGURATION ======
# Enable proxy support (set to 1 to allow proxy usage)
ALLOW_PROXIES=0

# HTTP proxy URL (if needed)
HTTP_PROXY=
HTTPS_PROXY=

# ====== RETRIES ======
# Default number of retries for transient errors
DEFAULT_RETRIES=2

# ====== PRECOMPUTED FAQ CACHE ======
# Enable precomputed FAQ cache
FAQ_CACHE_ENABLED=0
FAQ_CACHE_PATH=faq_cache.json

# Maximum query length (for DoS protection)
MAX_QUERY_LENGTH=1000000

# Query expansion file path
CLOCKIFY_QUERY_EXPANSIONS=config/query_expansions.json

# Maximum query expansion file size (in bytes)
MAX_QUERY_EXPANSION_FILE_SIZE=10485760
