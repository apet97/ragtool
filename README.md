# 1rag ‚Äì Clockify Support CLI v5.2

**Status**: üî® Under Active Improvement (Audit Improvements In Progress)
**Version**: 5.2 (Performance & Quality Improvements)
**Date**: 2025-11-06

> **Note**: Implementing audit recommendations. See [ANALYSIS_REPORT.md](ANALYSIS_REPORT.md) for details.

A local, stateless, closed-book Retrieval-Augmented Generation (RAG) chatbot for Clockify support documentation using Ollama.

**New in v5.1**: üîí Thread-safe, üöÄ 50-200ms faster first query, üõ†Ô∏è Better error messages, üîç Improved observability

## Platform Compatibility

| Platform | Status | Notes |
|----------|--------|-------|
| **Linux** | ‚úÖ Full Support | Recommended for production |
| **macOS Intel** | ‚úÖ Full Support | Uses IVFFlat FAISS index |
| **macOS Apple Silicon (M1/M2/M3)** | ‚úÖ **Full Support** | See [M1_COMPATIBILITY.md](M1_COMPATIBILITY.md) |
| **Windows** | ‚ö†Ô∏è WSL2 Recommended | Native support via WSL2 |

## Quick Start

### Installation

**Standard (Linux/macOS Intel)**:
```bash
python3 -m venv rag_env
source rag_env/bin/activate
pip install -r requirements.txt
```

**Apple Silicon (M1/M2/M3) - Recommended**:
```bash
# Use conda for best FAISS compatibility
conda create -n rag_env python=3.11
conda activate rag_env
conda install -c conda-forge faiss-cpu=1.8.0 numpy requests
conda install -c pytorch sentence-transformers pytorch
pip install urllib3==2.2.3 rank-bm25==0.2.2
```

For detailed M1 installation instructions, see [M1_COMPATIBILITY.md](M1_COMPATIBILITY.md).

### Build Knowledge Base
```bash
# One-time setup: build the vector index
python3 clockify_support_cli_final.py build knowledge_full.md
```

### Run
```bash
# Start interactive REPL
python3 clockify_support_cli_final.py chat

# With debug mode
python3 clockify_support_cli_final.py chat --debug

# Single query (supports --rerank/--json/--topk/--pack)
python3 clockify_support_cli_final.py ask "How do I track time in Clockify?" --rerank --json

# Run self-tests
python3 clockify_support_cli_final.py --selftest
```

## What's New in v4.1.2

### Apple Silicon M1/M2/M3 Support ‚úÖ

**ARM64 Optimizations**:
- ‚úÖ Automatic platform detection (`platform.machine()`)
- ‚úÖ FAISS FlatIP fallback for M1 (prevents segmentation faults)
- ‚úÖ Reduced FAISS cluster count (256‚Üí64) for ARM64 stability
- ‚úÖ PyTorch MPS acceleration support (2-3x faster embeddings)
- ‚úÖ 30-70% performance improvement over Intel Macs

**Installation Methods**:
- Conda recommended for M1 (best FAISS compatibility)
- Pip works with automatic fallback if FAISS fails
- Comprehensive troubleshooting guide in [M1_COMPATIBILITY.md](M1_COMPATIBILITY.md)

### Ollama-Only Architecture ‚úÖ

**Local Embeddings**:
- ‚úÖ SentenceTransformers integration (`all-MiniLM-L6-v2`)
- ‚úÖ Ollama embeddings via `nomic-embed-text` (fallback/alternative)
- ‚úÖ Configurable via `EMB_BACKEND` environment variable
- ‚úÖ FAISS ANN indexing for fast retrieval (<100ms)

**Hybrid Retrieval**:
- ‚úÖ BM25 sparse retrieval (exact keyword matching)
- ‚úÖ Dense semantic search (cosine similarity)
- ‚úÖ MMR diversification (Œª=0.7)
- ‚úÖ Dynamic snippet packing with token budget

**Production Features**:
- ‚úÖ Build lock with TTL (prevents concurrent builds)
- ‚úÖ Artifact versioning (auto-rebuild if KB changes)
- ‚úÖ Atomic file operations (fsync-safe)
- ‚úÖ Graceful FAISS fallback (full-scan if unavailable)
- ‚úÖ Comprehensive logging and KPI metrics

### CLI Enhancements

**Build Command**:
```bash
python3 clockify_support_cli_final.py build knowledge_full.md
```

**Chat Command**:
```bash
python3 clockify_support_cli_final.py chat [--debug] [--rerank]
```

**Ask Command** (single query):
```bash
python3 clockify_support_cli_final.py ask "Your question" [--rerank] [--topk 12] [--pack 6] [--threshold 0.30] [--json]
```

**Self-Test Command**:
```bash
python3 clockify_support_cli_final.py --selftest
```

## Files

### Production Code
- **clockify_support_cli_final.py** ‚Äì v4.1.2 production script (1,900+ lines)
- **knowledge_full.md** ‚Äì Clockify documentation knowledge base (~6.9 MB)

### Documentation

**Platform-Specific**
- `M1_COMPATIBILITY.md` ‚Äì Comprehensive Apple Silicon installation guide
- `M1_COMPATIBILITY_AUDIT.md` ‚Äì Technical audit report (v4.1.2)
- `M1_COMPREHENSIVE_AUDIT_2025.md` ‚Äì Complete compatibility analysis

**Getting Started**
- `README.md` (this file) ‚Äì Quick start and overview
- `CLAUDE.md` ‚Äì Architecture, common tasks, configuration guide
- `START_HERE.md` ‚Äì Entry point for new users
- `SUPPORT_CLI_QUICKSTART.md` ‚Äì 5-minute quick start

**Testing & Validation**
- `scripts/smoke.sh` ‚Äì Smoke test suite
- `scripts/acceptance_test.sh` ‚Äì Acceptance tests
- `TEST_GUIDE.md` ‚Äì Testing documentation

## Acceptance Tests (6/6 Pass)

1. ‚úÖ **Syntax Verification** ‚Äì Compiles with `python3 -m py_compile`
2. ‚úÖ **Help Flags (Global)** ‚Äì All 4 timeout flags present
3. ‚úÖ **Help Flags (Chat)** ‚Äì All 6 new chat flags present
4. ‚úÖ **Config Summary** ‚Äì Logged at startup with all parameters
5. ‚úÖ **Determinism Check** ‚Äì SHA256 hashes identical for same question (fixed seed)
6. ‚úÖ **Self-Tests** ‚Äì 4/4 unit tests pass (MMR sig, pack headroom, RTF guard, float32)

## Key Features

### Security üîí
- `allow_redirects=False` prevents auth header leaks
- `trust_env=False` by default (set `USE_PROXY=1` to enable)
- All POST calls use explicit (connect, read) timeouts
- Policy guardrails for sensitive queries

## DeepSeek Ollama Shim ‚Äì Production Guidance

The optional `deepseek_ollama_shim.py` exposes a minimal HTTP interface that is aimed at local development. Before enabling it in shared or production environments, review and apply the following controls:

### Risks
- ‚ö†Ô∏è **Unauthenticated access by default** ‚Äì anyone who can reach the port can call the DeepSeek API with your key.
- ‚ö†Ô∏è **Plain HTTP transport** ‚Äì requests are unencrypted unless you add TLS or terminate it behind a reverse proxy.
- ‚ö†Ô∏è **Limited rate limiting/auditing** ‚Äì no built-in throttling or logging.

### Hardening Checklist
- Set `SHIM_AUTH_TOKEN` to require a `Bearer` token (or `X-Auth-Token`) header on every request.
- Optionally configure `SHIM_ALLOW_IPS` with a comma-separated allowlist (e.g. `127.0.0.1,10.0.0.5`) to limit which clients can connect.
- Run the shim on a loopback or firewalled interface (`SHIM_HOST=127.0.0.1` by default) and expose it externally only via a gateway that enforces your organization‚Äôs security policies.

### Transport Security
- Provide `SHIM_TLS_CERT` and `SHIM_TLS_KEY` to enable built-in TLS termination. The server will refuse connections if the presented certificate is invalid.
- Alternatively, keep TLS and authentication in a hardened reverse proxy (e.g. Nginx, Caddy, Envoy) and run the shim behind it for additional observability and access controls.

Document and monitor any deployments that expose the shim outside of a trusted network segment.

### Reliability üõ°Ô∏è
- urllib3 v1 and v2 compatible retry adapter
- Manual bounded POST retry (max 1 retry, 0.5s backoff)
- Build lock with 10-minute mtime staleness detection
- All writes use atomic fsync-safe operations

### Correctness ‚úîÔ∏è
- Deterministic: `temperature=0, seed=42` on all LLM calls
- MMR signature fixed (no missing arguments)
- Headroom enforced: top-1 always included, budget respected
- float32 dtype guaranteed end-to-end

### Observability üëÅÔ∏è
- Config summary at startup
- One-line turn logging with latency metrics
- Rerank fallback logging
- Self-check unit tests

### Privacy Controls üîê
- `RAG_LOG_INCLUDE_ANSWER` (default `1`): set to `0`, `false`, or `no` to redact raw answers from `rag_queries.jsonl` logs.
- `RAG_LOG_ANSWER_PLACEHOLDER` (optional): customize the placeholder text written when answers are redacted. Leave unset to use `[REDACTED]`; set to an empty string to omit the field entirely.

## Configuration Examples

### Conservative (High Precision)
```bash
python3 clockify_support_cli.py chat \
  --threshold 0.50 \
  --pack 4 \
  --emb-read 180 \
  --chat-read 300
```

### Balanced (Defaults)
```bash
python3 clockify_support_cli.py chat
```

### Aggressive (High Recall)
```bash
python3 clockify_support_cli.py chat \
  --threshold 0.20 \
  --pack 8 \
  --rerank
```

### With Custom Timeouts
```bash
python3 clockify_support_cli.py chat \
  --emb-connect 5 --emb-read 180 \
  --chat-connect 5 --chat-read 300
```

### With Proxy
```bash
USE_PROXY=1 python3 clockify_support_cli.py chat
```

## Testing

### Run Full Test Suite (v5.1+)
```bash
# All tests with coverage
make test

# Or with pytest directly
pytest tests/ --cov=clockify_rag --cov=clockify_support_cli_final --cov-report=html
open htmlcov/index.html
```

### Run Self-Tests
```bash
python3 clockify_support_cli_final.py --selftest
# Expected: [selftest] 4/4 tests passed
```

### Run Thread Safety Tests (v5.1+)
```bash
# Verify thread safety with concurrent queries
pytest tests/test_thread_safety.py -v -n 4

# Integration tests
pytest tests/test_integration.py -v
```

### Run Determinism Check
```bash
python3 clockify_support_cli.py chat --det-check --seed 42
# Expected: [DETERMINISM] ... deterministic=true (both questions)
```

### Run with Debug Logging
```bash
python3 clockify_support_cli.py --log DEBUG chat
```

### Automated Evaluation

Ground-truth retrieval metrics are tracked via `eval.py`. The evaluator works in
both full and lightweight environments:

```bash
# Uses the populated dataset in eval_datasets/
python3 eval.py --dataset eval_datasets/clockify_v1.jsonl

# Enable verbose per-query breakdown
python3 eval.py --dataset eval_datasets/clockify_v1.jsonl --verbose
```

Key details:

- ‚úÖ **Hybrid mode** ‚Äì If the production index artifacts (`chunks.jsonl`,
  `vecs_n.npy`, `bm25.json`) are present, the script evaluates the full hybrid
  retrieval pipeline (`retrieve`).
- ‚úÖ **Lexical fallback** ‚Äì When embeddings are unavailable (e.g. in CI), the
  script re-chunks `knowledge_full.md` and evaluates using a BM25 index built
  with `rank-bm25`. Only lightweight dependencies are required: `numpy`,
  `requests`, `nltk`, and `rank-bm25`.
- ‚úÖ **Stable relevance labels** ‚Äì `eval_datasets/clockify_v1.jsonl` stores
  `title` + `section` pairs instead of raw chunk IDs so the dataset remains
  stable across rebuilds.
- ‚úÖ **Thresholds** ‚Äì Builds fail when any metric drops below the default
  thresholds: `MRR@10 ‚â• 0.70`, `Precision@5 ‚â• 0.55`, `NDCG@10 ‚â• 0.60`.

See [eval_datasets/README.md](eval_datasets/README.md) for dataset details and
label generation.

## Statistics

| Metric | Value |
|--------|-------|
| **Version** | 4.1.2 |
| **Production script** | clockify_support_cli_final.py |
| **Total lines** | ~1,900 |
| **Python version** | 3.9+ (3.11 recommended for M1) |
| **Dependencies** | 7 core + 1 optional (FAISS) |
| **Platform support** | Linux, macOS (Intel + M1), Windows (WSL2) |
| **Knowledge base size** | 6.9 MB (~150 pages) |
| **Embedding dimension** | 384 (all-MiniLM-L6-v2) |
| **Default chunks** | 384 chunks @ 1600 chars each |
| **Build time (M1)** | ~30 seconds |
| **Query latency (M1)** | ~6-11 seconds (LLM dominates) |
| **M1 performance gain** | 30-70% faster than Intel |

## Deployment Checklist

### Pre-Deployment
- [ ] Read `README.md` (this file)
- [ ] Read `M1_COMPATIBILITY.md` (if deploying on M1)
- [ ] Read `CLAUDE.md` (architecture overview)
- [ ] Install dependencies (see [Quick Start](#quick-start))
- [ ] Verify platform: `python3 -c "import platform; print(platform.machine())"`

### Installation
- [ ] Create virtual environment (`venv` or `conda`)
- [ ] Install dependencies from `requirements.txt` (or conda)
- [ ] Verify imports: `python3 -m py_compile clockify_support_cli_final.py`

### Build & Test
- [ ] Build knowledge base: `python3 clockify_support_cli_final.py build knowledge_full.md`
- [ ] Run self-tests: `python3 clockify_support_cli_final.py --selftest`
- [ ] Run smoke tests: `bash scripts/smoke.sh`
- [ ] Test query: `python3 clockify_support_cli_final.py ask "How do I track time?"`

### Validation (M1 Only)
- [ ] Verify ARM64 detection in build logs: `"macOS arm64 detected"`
- [ ] Check PyTorch MPS: `python3 -c "import torch; print(torch.backends.mps.is_available())"`
- [ ] Run M1 compatibility tests: `bash scripts/m1_compatibility_test.sh` (if available)

### Production (v5.1+)
- [ ] Configure Ollama endpoint (`OLLAMA_URL` env var)
- [ ] Set production timeouts (if needed)
- [ ] Test end-to-end query
- [ ] **Choose deployment mode** (see below)
- [ ] Deploy to production
- [ ] Monitor first queries for errors

#### Thread-Safe Deployment (v5.1+)

**Option 1: Multi-threaded (RECOMMENDED)**
```bash
# Deploy with multi-worker, multi-threaded processes
gunicorn -w 4 --threads 4 app:app

# Or with uvicorn
uvicorn app:app --workers 4
```
- Thread safety locks protect shared state (QueryCache, RateLimiter, _FAISS_INDEX)
- Cache and rate limiter shared across threads within same process
- Better resource utilization

**Option 2: Single-threaded (legacy)**
```bash
# Deploy with single-worker processes
gunicorn -w 4 --threads 1 app:app
```
- Each worker has its own process memory (no shared state)
- Cache and rate limiter per-process

## Documentation Reading Paths

### üöÄ For Immediate Deployment (5 min)
1. `README.md` (this file) ‚Äì Quick start
2. Install dependencies (see [Quick Start](#quick-start))
3. Build & test: `python3 clockify_support_cli_final.py build knowledge_full.md`
4. Run: `python3 clockify_support_cli_final.py chat`

### üçé For Apple Silicon M1/M2/M3 (10 min)
1. `M1_COMPATIBILITY.md` ‚Äì Installation guide
2. `M1_COMPREHENSIVE_AUDIT_2025.md` ‚Äì Full compatibility analysis
3. Use conda for installation (recommended)

### üèóÔ∏è For Architecture (30 min)
1. `CLAUDE.md` ‚Äì Architecture & development guide
2. `clockify_support_cli_final.py` ‚Äì Read code comments
3. `M1_COMPATIBILITY.md` ‚Äì Platform-specific optimizations

### ‚úÖ For Testing & Validation (15 min)
1. Run: `python3 clockify_support_cli_final.py --selftest`
2. Run: `bash scripts/smoke.sh`
3. Run: `bash scripts/acceptance_test.sh`

## Requirements

### Core Dependencies
- **Python 3.9+** (3.11 recommended for M1 Macs)
- **numpy** 2.3.4 ‚Äì Vector operations and embeddings
- **requests** 2.32.5 ‚Äì HTTP client for Ollama API
- **urllib3** 2.2.3 ‚Äì Low-level HTTP
- **sentence-transformers** 3.3.1 ‚Äì Local embedding generation
- **torch** 2.4.2 ‚Äì PyTorch for neural models
- **rank-bm25** 0.2.2 ‚Äì Sparse retrieval (BM25 algorithm)

### Optional Dependencies
- **faiss-cpu** 1.8.0.post1 ‚Äì Fast approximate nearest neighbors (ANN)
  - Required for optimal performance
  - **M1 Users**: Install via conda for best compatibility
  - Graceful fallback to full-scan if unavailable

### External Services
- **Ollama** ‚Äì Local LLM server
  - Default endpoint: `http://127.0.0.1:11434`
  - Required models:
    - `nomic-embed-text` ‚Äì 768-dim embeddings (optional if using local)
    - `qwen2.5:32b` ‚Äì LLM for answer generation

### Installation
See [Quick Start](#quick-start) section above or [M1_COMPATIBILITY.md](M1_COMPATIBILITY.md) for M1-specific instructions.

## Environment Variables

### Core Configuration
```bash
OLLAMA_URL              # Ollama endpoint (default: http://127.0.0.1:11434)
GEN_MODEL               # Generation model (default: qwen2.5:32b)
EMB_MODEL               # Embedding model (default: nomic-embed-text)
EMB_BACKEND             # "local" or "ollama" (default: local)
```

### Performance Tuning
```bash
CTX_BUDGET              # Context token budget (default: 2800)
ANN                     # ANN backend: "faiss" or "none" (default: faiss)
ANN_NLIST               # FAISS clusters (default: 64 for M1, 256 otherwise)
ANN_NPROBE              # FAISS search clusters (default: 16)
ALPHA                   # Hybrid retrieval weight (default: 0.5)
```

### Timeouts
```bash
EMB_CONNECT_TIMEOUT     # Embedding connect timeout (default: 3)
EMB_READ_TIMEOUT        # Embedding read timeout (default: 60)
CHAT_CONNECT_TIMEOUT    # Chat connect timeout (default: 3)
CHAT_READ_TIMEOUT       # Chat read timeout (default: 120)
RERANK_READ_TIMEOUT     # Rerank timeout (default: 180)
```

### M1-Specific
```bash
# Force FAISS fallback (useful if FAISS crashes on M1)
export USE_ANN=none

# Enable PyTorch MPS fallback
export PYTORCH_ENABLE_MPS_FALLBACK=1
```

## Production Status

‚úÖ **READY FOR IMMEDIATE DEPLOYMENT**

**v4.1.2 Features**:
- ‚úÖ Full M1/M2/M3 Apple Silicon support
- ‚úÖ Ollama-optimized architecture
- ‚úÖ Local embeddings (SentenceTransformers)
- ‚úÖ Hybrid retrieval (BM25 + dense + MMR)
- ‚úÖ FAISS ANN indexing with graceful fallback
- ‚úÖ Comprehensive testing suite
- ‚úÖ Production-grade error handling
- ‚úÖ Platform-specific optimizations

**Compatibility**:
- ‚úÖ Backward compatible with v3.x
- ‚úÖ Works on Linux, macOS (Intel + M1), Windows (WSL2)
- ‚úÖ Graceful degradation (FAISS optional)
- ‚úÖ 30-70% performance improvement on M1

## Support

For questions about:
- **Quick Start**: See [Quick Start](#quick-start) above
- **M1 Installation**: See `M1_COMPATIBILITY.md`
- **Architecture**: See `CLAUDE.md`
- **Testing**: Run `python3 clockify_support_cli_final.py --selftest`
- **Troubleshooting**: See `M1_COMPATIBILITY.md` (troubleshooting section)

### Common Issues

**FAISS import error on M1**:
```bash
# Solution: Use conda
conda install -c conda-forge faiss-cpu=1.8.0
```

**Slow performance**:
```bash
# Check if running native ARM (not Rosetta)
python3 -c "import platform; print(platform.machine())"
# Expected on M1: arm64
```

**Missing dependencies**:
```bash
pip install -r requirements.txt
# Or for M1: see M1_COMPATIBILITY.md
```

---

**Version**: 4.1.2 (Ollama-Optimized with M1 Support)
**Date**: 2025-11-05
**Status**: üöÄ **PRODUCTION-READY**
**Platform**: Linux | macOS (Intel + Apple Silicon) | Windows (WSL2)
