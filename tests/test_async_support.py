"""
Tests for async_support module.

Tests async versions of:
- async_embed_query
- async_ask_llm
- async_generate_llm_answer
- async_answer_once

Critical coverage areas:
1. Happy path - async functions return valid results
2. Concurrency safety - multiple concurrent calls don't corrupt shared state
3. Timeout handling - slow operations raise appropriate exceptions
4. Error propagation - LLM errors are handled correctly
5. Equivalence - async and sync versions return identical results
6. Performance - async provides speedup for concurrent queries
"""

import asyncio
import time
from unittest.mock import patch

import pytest
import numpy as np

from clockify_rag.async_support import (
    async_embed_query,
    async_ask_llm,
    async_generate_llm_answer,
    async_answer_once,
)
from clockify_rag.answer import answer_once
from clockify_rag.config import REFUSAL_STR
from clockify_rag.exceptions import LLMError, LLMUnavailableError


class TestAsyncEmbedQuery:
    """Test async_embed_query function."""

    @pytest.mark.asyncio
    async def test_async_embed_query_happy_path(self):
        """Test async_embed_query returns normalized embedding."""
        text = "How do I track time?"

        result = await async_embed_query(text)

        assert isinstance(result, np.ndarray)
        assert result.dtype == np.float32
        # Check normalization (L2 norm should be ~1.0)
        norm = np.linalg.norm(result)
        assert 0.99 <= norm <= 1.01

    @pytest.mark.asyncio
    async def test_async_embed_query_empty_text(self):
        """Test async_embed_query handles empty text."""
        text = ""

        result = await async_embed_query(text)

        assert isinstance(result, np.ndarray)
        # Should still return normalized vector (mock client handles this)
        norm = np.linalg.norm(result)
        assert 0.99 <= norm <= 1.01

    @pytest.mark.asyncio
    async def test_async_embed_query_concurrent_calls(self):
        """Test multiple concurrent embedding calls don't interfere."""
        texts = [f"Query {i}" for i in range(20)]

        # Fire 20 concurrent embedding requests
        results = await asyncio.gather(*[async_embed_query(text) for text in texts])

        assert len(results) == 20
        for result in results:
            assert isinstance(result, np.ndarray)
            norm = np.linalg.norm(result)
            assert 0.99 <= norm <= 1.01

    @pytest.mark.asyncio
    async def test_async_embed_query_error_handling(self):
        """Test async_embed_query raises LLMError on failure."""
        with patch("clockify_rag.async_support.get_llm_client") as mock_client:
            mock_client.return_value.create_embedding.side_effect = RuntimeError("Embedding service down")

            with pytest.raises(LLMError) as exc_info:
                await async_embed_query("test query")

            assert "Embedding failed" in str(exc_info.value)


class TestAsyncAskLLM:
    """Test async_ask_llm function."""

    @pytest.mark.asyncio
    async def test_async_ask_llm_happy_path(self):
        """Test async_ask_llm returns LLM response text."""
        question = "How do I track time?"
        context_block = "[1] Track time using the timer button."

        result = await async_ask_llm(question, context_block)

        assert isinstance(result, str)
        assert len(result) > 0  # Mock client returns non-empty response

    @pytest.mark.asyncio
    async def test_async_ask_llm_with_json_response(self):
        """Test async_ask_llm handles JSON response from LLM."""
        question = "How do I track time?"
        context_block = "[1] Track time using the timer button."

        # Mock LLM client to return JSON
        with patch("clockify_rag.async_support.get_llm_client") as mock_client:
            mock_response = {
                "message": {"content": '{"answer": "Track time with timer button [1].", "confidence": 85}'},
                "done": True,
            }
            mock_client.return_value.chat_completion.return_value = mock_response

            result = await async_ask_llm(question, context_block)

            assert isinstance(result, str)
            assert "answer" in result  # Raw JSON string returned

    @pytest.mark.asyncio
    async def test_async_ask_llm_concurrent_calls(self):
        """Test multiple concurrent LLM calls don't corrupt state."""
        questions = [f"Question {i}" for i in range(20)]
        context = "[1] Sample context for testing."

        # Fire 20 concurrent LLM requests
        results = await asyncio.gather(*[async_ask_llm(q, context) for q in questions])

        assert len(results) == 20
        for result in results:
            assert isinstance(result, str)
            assert len(result) > 0

    @pytest.mark.asyncio
    async def test_async_ask_llm_unavailable_error(self):
        """Test async_ask_llm propagates LLMUnavailableError."""
        with patch("clockify_rag.async_support.get_llm_client") as mock_client:
            mock_client.return_value.chat_completion.side_effect = LLMUnavailableError("LLM service down")

            with pytest.raises(LLMUnavailableError):
                await async_ask_llm("test question", "test context")

    @pytest.mark.asyncio
    async def test_async_ask_llm_generic_error(self):
        """Test async_ask_llm converts generic errors to LLMError."""
        with patch("clockify_rag.async_support.get_llm_client") as mock_client:
            mock_client.return_value.chat_completion.side_effect = RuntimeError("Network error")

            with pytest.raises(LLMError) as exc_info:
                await async_ask_llm("test question", "test context")

            assert "LLM generation failed" in str(exc_info.value)


class TestAsyncGenerateLLMAnswer:
    """Test async_generate_llm_answer function."""

    @pytest.mark.asyncio
    async def test_async_generate_llm_answer_happy_path(self):
        """Test async_generate_llm_answer returns answer, timing, and confidence."""
        question = "How do I track time?"
        context_block = "[1] Track time using the timer button."
        packed_ids = [1]

        answer, timing, confidence = await async_generate_llm_answer(question, context_block, packed_ids=packed_ids)

        assert isinstance(answer, str)
        assert isinstance(timing, float)
        assert timing > 0  # Should take some time
        # Confidence may be None if not in response
        assert confidence is None or isinstance(confidence, int)

    @pytest.mark.asyncio
    async def test_async_generate_llm_answer_json_parsing(self):
        """Test async_generate_llm_answer parses JSON response with confidence."""
        question = "How do I track time?"
        context_block = "[1] Track time using the timer button."

        # Mock LLM to return JSON with confidence
        with patch("clockify_rag.async_support.async_ask_llm") as mock_ask:
            mock_ask.return_value = '{"answer": "Track time with timer [1].", "confidence": 85}'

            answer, timing, confidence = await async_generate_llm_answer(question, context_block, packed_ids=[1])

            assert answer == "Track time with timer [1]."
            assert confidence == 85
            assert isinstance(timing, float)

    @pytest.mark.asyncio
    async def test_async_generate_llm_answer_json_with_code_fence(self):
        """Test async_generate_llm_answer handles JSON in code fences."""
        question = "How do I track time?"
        context_block = "[1] Track time using the timer button."

        # Mock LLM to return JSON wrapped in code fence
        with patch("clockify_rag.async_support.async_ask_llm") as mock_ask:
            mock_ask.return_value = '```json\n{"answer": "Track time with timer [1].", "confidence": 90}\n```'

            answer, timing, confidence = await async_generate_llm_answer(question, context_block, packed_ids=[1])

            assert answer == "Track time with timer [1]."
            assert confidence == 90

    @pytest.mark.asyncio
    async def test_async_generate_llm_answer_citation_validation_strict(self):
        """Test citation validation in strict mode rejects missing citations."""
        question = "How do I track time?"
        context_block = "[1] Track time using the timer button."

        # Mock LLM to return answer without citations
        with patch("clockify_rag.async_support.async_ask_llm") as mock_ask:
            mock_ask.return_value = "Track time with the timer button."

            with patch("clockify_rag.config.STRICT_CITATIONS", True):
                answer, timing, confidence = await async_generate_llm_answer(question, context_block, packed_ids=[1])

                # Should refuse in strict mode
                assert answer == REFUSAL_STR
                assert confidence is None

    @pytest.mark.asyncio
    async def test_async_generate_llm_answer_invalid_citations_strict(self):
        """Test citation validation rejects invalid citation IDs in strict mode."""
        question = "How do I track time?"
        context_block = "[1] Track time using the timer button."

        # Mock LLM to return answer with invalid citations
        with patch("clockify_rag.async_support.async_ask_llm") as mock_ask:
            mock_ask.return_value = "Track time with timer [999]."  # ID 999 not in packed_ids

            with patch("clockify_rag.config.STRICT_CITATIONS", True):
                answer, timing, confidence = await async_generate_llm_answer(question, context_block, packed_ids=[1])

                # Should refuse in strict mode
                assert answer == REFUSAL_STR
                assert confidence is None

    @pytest.mark.asyncio
    async def test_async_generate_llm_answer_confidence_out_of_range(self):
        """Test confidence values outside 0-100 are ignored."""
        question = "How do I track time?"
        context_block = "[1] Track time using the timer button."

        # Mock LLM to return invalid confidence
        with patch("clockify_rag.async_support.async_ask_llm") as mock_ask:
            mock_ask.return_value = '{"answer": "Track time [1].", "confidence": 150}'

            answer, timing, confidence = await async_generate_llm_answer(question, context_block, packed_ids=[1])

            assert answer == "Track time [1]."
            assert confidence is None  # Out of range confidence ignored


class TestAsyncAnswerOnce:
    """Test async_answer_once complete pipeline."""

    @pytest.mark.asyncio
    async def test_async_answer_once_happy_path(self, sample_chunks, sample_embeddings, sample_bm25):
        """Test async_answer_once returns complete answer result."""
        question = "How do I track time?"

        # Mock embed_query to avoid sentence_transformers dependency
        with patch("clockify_rag.retrieval.embed_query") as mock_embed:
            mock_embed.return_value = sample_embeddings[0]  # Use first embedding as query

            result = await async_answer_once(question, sample_chunks, sample_embeddings, sample_bm25)

            assert isinstance(result, dict)
            assert "answer" in result
            assert "confidence" in result
            assert "timing" in result
            assert "metadata" in result
            assert "routing" in result
            assert isinstance(result["answer"], str)
            assert result["timing"]["total_ms"] > 0

    @pytest.mark.asyncio
    async def test_async_answer_once_concurrent_queries(self, sample_chunks, sample_embeddings, sample_bm25):
        """Test concurrent async queries don't corrupt shared state."""
        questions = [f"How do I track time? {i}" for i in range(20)]

        # Mock embed_query to avoid sentence_transformers dependency
        with patch("clockify_rag.retrieval.embed_query") as mock_embed:
            mock_embed.return_value = sample_embeddings[0]  # Use first embedding as query

            # Fire 20 concurrent queries
            results = await asyncio.gather(
                *[async_answer_once(q, sample_chunks, sample_embeddings, sample_bm25) for q in questions]
            )

            assert len(results) == 20
            for result in results:
                assert isinstance(result, dict)
                assert "answer" in result
                assert isinstance(result["answer"], str)
                # No NoneType errors, no corrupted state
                assert result["timing"]["total_ms"] > 0

    @pytest.mark.asyncio
    async def test_async_answer_once_empty_retrieval(self, sample_chunks, sample_embeddings, sample_bm25):
        """Test async_answer_once handles coverage failure gracefully."""
        question = "Irrelevant question about quantum physics"

        # Mock retrieve to return low-scoring chunks (patch at the import source)
        with patch("clockify_rag.retrieval.retrieve") as mock_retrieve:
            mock_retrieve.return_value = ([], {"dense": np.array([]), "bm25": np.array([])})

            result = await async_answer_once(question, sample_chunks, sample_embeddings, sample_bm25)

            assert result["answer"] == REFUSAL_STR
            assert result["refused"] is True
            assert result["confidence"] is None
            assert result["metadata"]["coverage_check"] == "failed"

    @pytest.mark.asyncio
    async def test_async_answer_once_llm_unavailable(self, sample_chunks, sample_embeddings, sample_bm25):
        """Test async_answer_once handles LLM unavailable error."""
        question = "How do I track time?"

        # Mock embed_query and async_generate_llm_answer
        with patch("clockify_rag.retrieval.embed_query") as mock_embed:
            mock_embed.return_value = sample_embeddings[0]

            with patch("clockify_rag.async_support.async_generate_llm_answer") as mock_generate:
                mock_generate.side_effect = LLMUnavailableError("LLM service down")

                result = await async_answer_once(question, sample_chunks, sample_embeddings, sample_bm25)

                assert result["answer"] == REFUSAL_STR
                assert result["refused"] is True
                assert result["metadata"]["llm_error"] == "llm_unavailable"
                assert "LLM service down" in result["metadata"]["llm_error_msg"]

    @pytest.mark.asyncio
    async def test_async_answer_once_llm_error(self, sample_chunks, sample_embeddings, sample_bm25):
        """Test async_answer_once handles generic LLM error."""
        question = "How do I track time?"

        # Mock embed_query and async_generate_llm_answer
        with patch("clockify_rag.retrieval.embed_query") as mock_embed:
            mock_embed.return_value = sample_embeddings[0]

            with patch("clockify_rag.async_support.async_generate_llm_answer") as mock_generate:
                mock_generate.side_effect = LLMError("Network timeout")

                result = await async_answer_once(question, sample_chunks, sample_embeddings, sample_bm25)

                assert result["answer"] == REFUSAL_STR
                assert result["refused"] is True
                assert result["metadata"]["llm_error"] == "llm_error"
                assert "Network timeout" in result["metadata"]["llm_error_msg"]


class TestAsyncVsSyncEquivalence:
    """Test that async and sync versions return equivalent results."""

    @pytest.mark.asyncio
    async def test_async_sync_equivalence(self, sample_chunks, sample_embeddings, sample_bm25):
        """Test async_answer_once and answer_once return identical results."""
        question = "How do I track time?"
        seed = 42  # Fixed seed for reproducibility

        # Mock embed_query for both sync and async calls
        with patch("clockify_rag.retrieval.embed_query") as mock_embed:
            mock_embed.return_value = sample_embeddings[0]

            # Get sync result
            sync_result = answer_once(question, sample_chunks, sample_embeddings, sample_bm25, seed=seed)

            # Get async result
            async_result = await async_answer_once(question, sample_chunks, sample_embeddings, sample_bm25, seed=seed)

            # Compare key fields (timing will differ slightly)
            assert sync_result["answer"] == async_result["answer"]
            assert sync_result["refused"] == async_result["refused"]
            assert sync_result["confidence"] == async_result["confidence"]
            assert len(sync_result["selected_chunks"]) == len(async_result["selected_chunks"])
            assert len(sync_result["packed_chunks"]) == len(async_result["packed_chunks"])


class TestAsyncPerformance:
    """Test async performance benefits for concurrent queries."""

    @pytest.mark.asyncio
    async def test_async_concurrent_speedup(self, sample_chunks, sample_embeddings, sample_bm25):
        """Test async provides speedup for concurrent queries."""
        questions = [f"Question {i}" for i in range(10)]

        # Mock embed_query for both sync and async calls
        with patch("clockify_rag.retrieval.embed_query") as mock_embed:
            mock_embed.return_value = sample_embeddings[0]

            # Measure async time
            async_start = time.time()
            async_results = await asyncio.gather(
                *[async_answer_once(q, sample_chunks, sample_embeddings, sample_bm25) for q in questions]
            )
            async_time = time.time() - async_start

            # Measure sequential sync time (simulate serial execution)
            sync_start = time.time()
            sync_results = []
            for q in questions:
                result = answer_once(q, sample_chunks, sample_embeddings, sample_bm25)
                sync_results.append(result)
            sync_time = time.time() - sync_start

            # Async should be faster for concurrent queries
            # Note: In test environment with mock client, speedup may be modest
            # In production with real LLM, expect 2-4x speedup
            assert len(async_results) == 10
            assert len(sync_results) == 10

            # Basic sanity check - async shouldn't be slower than sync
            # Allow 2x overhead for test environment
            assert async_time < sync_time * 2.0

    def test_async_no_event_loop_error(self, sample_chunks, sample_embeddings, sample_bm25):
        """Test that async functions return coroutine objects."""
        question = "How do I track time?"

        # Calling async function without await returns a coroutine object
        result = async_answer_once(question, sample_chunks, sample_embeddings, sample_bm25)

        # Should be a coroutine, not the actual result
        import inspect

        assert inspect.iscoroutine(result)

        # Clean up the coroutine to avoid warning
        result.close()
