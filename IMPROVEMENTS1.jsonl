{"rank":1,"category":"RAG","subcategory":"evaluation","issue":"No ground truth evaluation dataset exists","impact":"HIGH","effort":"LOW","file":"eval.py","line":89,"current":"Evaluation script exists but no eval_dataset.jsonl file with ground truth","proposed":"Create 50-100 question-answer-chunk triplets with relevance labels","rationale":"Without ground truth, cannot measure retrieval quality (MRR, NDCG, P@K), validate improvements, or perform A/B testing. This blocks systematic RAG optimization.","implementation":"1. Sample 50-100 representative questions from user logs or documentation 2. For each question, manually identify relevant chunk IDs 3. Add difficulty labels (easy/medium/hard) 4. Format as JSONL: {\"query\": \"...\", \"relevant_chunk_ids\": [...], \"difficulty\": \"...\"}","expected_gain":"Enables all downstream improvements: metric tracking, A/B testing, reranking validation, prompt tuning","references":["https://arxiv.org/abs/2104.08663"]}
{"rank":2,"category":"correctness","subcategory":"concurrency","issue":"Global state not thread-safe (QueryCache, RateLimiter, _FAISS_INDEX)","impact":"HIGH","effort":"MEDIUM","file":"caching.py","line":72,"current":"No locks protecting shared state (cache dict, access_order deque, requests deque, _FAISS_INDEX)","proposed":"Add threading.Lock to protect all shared state modifications","rationale":"Race conditions in multi-threaded deployments can cause cache corruption, rate limit bypasses, FAISS load conflicts. Critical for production with concurrent requests.","implementation":"1. Add self._lock = threading.Lock() to __init__ 2. Wrap all cache/limiter methods with 'with self._lock:' 3. Protect _FAISS_INDEX loading with module-level lock 4. Add thread safety tests","expected_gain":"Enables safe multi-threaded deployment, prevents cache corruption and race conditions","references":["https://docs.python.org/3/library/threading.html#threading.Lock"]}
{"rank":3,"category":"RAG","subcategory":"reranking","issue":"Cross-encoder reranking disabled by default (use_rerank=False)","impact":"HIGH","effort":"LOW","file":"clockify_support_cli_final.py","line":2381,"current":"LLM-based reranking available but disabled, only MMR diversification used","proposed":"Enable cross-encoder reranking by default with efficient model (e.g., cross-encoder/ms-marco-MiniLM-L6-v2)","rationale":"Cross-encoder reranking improves P@5 by 10-15% and answer quality by 5-10% with minimal latency cost (<50ms for 12 candidates). Current LLM reranking is slower and less accurate.","implementation":"1. Add sentence-transformers cross-encoder to requirements 2. Implement CrossEncoderReranker class 3. Set use_rerank=True in config 4. Benchmark latency vs. accuracy tradeoff","expected_gain":"10-15% improvement in P@5, 5-10% improvement in answer quality, <50ms latency per query","references":["https://www.sbert.net/examples/applications/cross-encoder/README.html"]}
{"rank":4,"category":"testing","subcategory":"coverage","issue":"Missing tests for retrieval pipeline (hybrid, MMR, reranking)","impact":"HIGH","effort":"MEDIUM","file":"tests/","line":null,"current":"Only 8 test files covering ~20% of code (chunking, BM25, sanitization, cache, rate limiter)","proposed":"Add comprehensive tests for retrieval.py, mmr.py, reranking.py with fixtures","rationale":"Retrieval pipeline is critical path but untested. Regressions in hybrid scoring, MMR diversification, or reranking would go undetected until production.","implementation":"1. Create tests/test_retrieval.py with fixtures for chunks/embeddings 2. Test hybrid scoring correctness (BM25 + dense blend) 3. Test MMR increases diversity 4. Test reranking improves ranking 5. Add integration tests for end-to-end pipeline","expected_gain":"Prevent regressions in critical path, enable confident refactoring, catch bugs before production","references":["https://pytest.org/en/stable/how-to/fixtures.html"]}
{"rank":5,"category":"RAG","subcategory":"retrieval","issue":"Query expansion asymmetry (dense retrieval doesn't benefit from synonyms)","impact":"MEDIUM","effort":"LOW","file":"clockify_support_cli_final.py","line":1401,"current":"expand_query() only applied to BM25, dense retrieval uses original question","proposed":"Apply query expansion to dense retrieval via embedding averaging of question + top-3 synonyms","rationale":"Dense retrieval should benefit from domain synonyms just like BM25. Current approach limits semantic expansion. Embedding averaging (0.7 question + 0.3 synonyms) improves recall by 5-10%.","implementation":"1. Add expand_dense_query() function 2. Embed question + top-3 synonyms 3. Weighted average: 0.7*q_embed + 0.1*syn1 + 0.1*syn2 + 0.1*syn3 4. Use averaged embedding for dense retrieval","expected_gain":"5-10% improvement in recall for semantic queries, especially domain-specific terms","references":["https://arxiv.org/abs/1911.03814"]}
{"rank":6,"category":"performance","subcategory":"initialization","issue":"Lazy FAISS loading adds first-query latency penalty","impact":"MEDIUM","effort":"LOW","file":"clockify_support_cli_final.py","line":1407,"current":"FAISS index loaded on first query (_FAISS_INDEX = None initially)","proposed":"Preload FAISS index in load_index() to amortize loading across all queries","rationale":"First query pays 50-200ms latency penalty for FAISS loading. Preloading eliminates this penalty and provides consistent query latency.","implementation":"1. In load_index(), check if USE_ANN == 'faiss' 2. Load index immediately: _FAISS_INDEX = load_faiss_index(FILES['faiss_index']) 3. Set nprobe: _FAISS_INDEX.nprobe = ANN_NPROBE 4. Log preload success","expected_gain":"50-200ms first-query speedup, consistent query latency","references":[]}
{"rank":7,"category":"correctness","subcategory":"locking","issue":"Build lock deadline not respected (reset on each retry)","impact":"MEDIUM","effort":"LOW","file":"utils.py","line":585,"current":"deadline = time.time() + 10.0 at start, but inner loop uses end = time.time() + 10.0 (resets)","proposed":"Use deadline consistently: while time.time() < deadline instead of resetting end","rationale":"Current code can hang longer than 10s (up to 10s per retry). User expects bounded wait time. Simple fix ensures deadline is respected.","implementation":"Replace 'end = time.time() + 10.0' with direct deadline check: 'while time.time() < deadline'. Remove end variable.","expected_gain":"Guarantees lock acquisition completes within 10s, improves user experience","references":[]}
{"rank":8,"category":"performance","subcategory":"optimization","issue":"Redundant score normalization for full corpus even when using ANN","impact":"MEDIUM","effort":"LOW","file":"clockify_support_cli_final.py","line":1438,"current":"Normalizes full dense_scores_full and bm_scores_full even when FAISS/HNSW returns candidates","proposed":"Only normalize scores for candidates when using ANN, normalize full corpus only for exhaustive search","rationale":"When using FAISS (top-k from 10k+ candidates), normalizing all 10k scores is wasteful. Only need to normalize the ~200 candidate scores. Saves 10-20ms per query.","implementation":"1. Check if _FAISS_INDEX or hnsw is active 2. If yes: extract candidate scores, normalize only those 3. If no: normalize full corpus (current behavior) 4. Reuse normalized candidates for hybrid scoring","expected_gain":"10-20ms per query when using FAISS/HNSW, especially on large corpora (10k+ chunks)","references":[]}
{"rank":9,"category":"RAG","subcategory":"validation","issue":"No answer validation (citation checking, hallucination detection)","impact":"MEDIUM","effort":"MEDIUM","file":"clockify_support_cli_final.py","line":1642,"current":"LLM answer accepted as-is, no validation of citations or content","proposed":"Add post-processing to validate citations exist, detect hallucination patterns","rationale":"LLM can hallucinate citations ([id_fake]) or provide info not in snippets. Validation improves trust and catches errors before returning to user.","implementation":"1. Parse citations from answer (regex: \\[(id_[^\\]]+)\\]) 2. Verify all cited IDs exist in provided snippets 3. Check for hallucination patterns (generic phrases, speculation) 4. If validation fails, return refusal or flag answer","expected_gain":"Reduced hallucination rate, improved citation accuracy, better user trust","references":["https://arxiv.org/abs/2305.14627"]}
{"rank":10,"category":"code_quality","subcategory":"duplication","issue":"Duplicate code between monolithic file and modular package","impact":"MEDIUM","effort":"HIGH","file":"clockify_support_cli_final.py","line":1,"current":"2,857-line monolithic file duplicates logic from clockify_rag/ package modules","proposed":"Deprecate monolithic file, migrate all code to modular package, update imports","rationale":"Code duplication causes maintenance burden (fix bugs twice), inconsistencies (batch sizes differ), and confusion. Modular package is cleaner and tested separately.","implementation":"1. Move remaining functions (MMR, LLM, CLI) to package modules 2. Update all imports in benchmark.py, eval.py, tests 3. Add deprecation warning to monolithic file 4. Delete after migration complete","expected_gain":"Reduced maintenance burden, consistent behavior, easier testing, better modularity","references":[]}
{"rank":11,"category":"performance","subcategory":"bm25","issue":"BM25 early termination threshold too conservative","impact":"LOW","effort":"LOW","file":"indexing.py","line":163,"current":"Early termination only when len(doc_lens) > top_k * 2 (threshold factor = 2)","proposed":"Lower threshold to top_k * 1.5 to enable early termination sooner","rationale":"Current threshold requires 24+ docs for top_k=12. Many corpora have 2k-10k chunks where early termination would help. Lowering to 1.5 enables speedup for smaller corpora with minimal accuracy loss (<1%).","implementation":"Change condition from 'if top_k is not None and top_k > 0 and len(doc_lens) > top_k * 2' to '... > top_k * 1.5'. Benchmark accuracy impact.","expected_gain":"2-3x BM25 speedup on corpora with 1k-10k chunks, <1% accuracy loss","references":["https://arxiv.org/abs/1206.4621"]}
{"rank":12,"category":"testing","subcategory":"integration","issue":"No end-to-end integration tests for full RAG pipeline","impact":"MEDIUM","effort":"MEDIUM","file":"tests/","line":null,"current":"Only unit tests for individual components (chunking, BM25, cache), no integration tests","proposed":"Add integration test: build index from sample MD → query → verify answer contains expected content","rationale":"Integration tests catch bugs in component interactions (e.g., embedding dimension mismatch, chunk ID mismatch). Critical for validating full pipeline correctness.","implementation":"1. Create tests/test_integration.py 2. Use small sample KB (10-20 chunks) 3. Test: build → query → parse answer → verify citations 4. Test incremental build with cache 5. Test concurrent queries (if thread-safe)","expected_gain":"Catch integration bugs early, validate end-to-end correctness, enable confident releases","references":[]}
{"rank":13,"category":"RAG","subcategory":"confidence","issue":"Confidence scoring not calibrated against ground truth","impact":"MEDIUM","effort":"MEDIUM","file":"clockify_support_cli_final.py","line":742,"current":"LLM returns confidence 0-100, but not calibrated (no correlation with actual accuracy)","proposed":"Collect (confidence, accuracy) pairs from evaluation, fit calibration curve, apply at inference","rationale":"Uncalibrated confidence misleads users (e.g., 90% confidence but 50% actual accuracy). Calibration via isotonic regression or Platt scaling improves trust and refusal decisions.","implementation":"1. Collect predictions with confidence on eval dataset 2. Compute accuracy per confidence bin 3. Fit isotonic regression: sklearn.isotonic.IsotonicRegression 4. Apply calibration: calibrated_conf = calibrator.predict([raw_conf]) 5. Use calibrated confidence for refusal threshold","expected_gain":"Better refusal decisions, improved user trust, reduced over-confidence","references":["https://scikit-learn.org/stable/modules/calibration.html"]}
{"rank":14,"category":"performance","subcategory":"batching","issue":"No batching for multiple queries (REPL processes one at a time)","impact":"LOW","effort":"MEDIUM","file":"clockify_support_cli_final.py","line":2373,"current":"answer_once() processes single query, no batch support","proposed":"Add answer_batch() to process multiple queries in parallel (embed batch, retrieve parallel)","rationale":"Batch processing improves throughput 2-3x by amortizing embedding and HTTP overhead. Useful for bulk queries or API endpoints.","implementation":"1. Add answer_batch(questions: list, ...) → list 2. Batch embed questions: embed_local_batch(questions) 3. Parallel retrieval for each 4. Batch LLM calls if supported 5. Return list of (answer, metadata) tuples","expected_gain":"2-3x throughput for batch queries, lower latency per query","references":[]}
{"rank":15,"category":"code_quality","subcategory":"types","issue":"Incomplete type hints (generic tuple, dict without element types)","impact":"LOW","effort":"LOW","file":"*.py","line":null,"current":"Type hints use generic tuple, dict, list without specifying element types","proposed":"Add full type hints: tuple[int, str], dict[str, float], list[dict], etc.","rationale":"Generic types provide no type safety. Specific types enable better IDE support, catch bugs at type-check time, improve code documentation.","implementation":"1. Review all function signatures 2. Replace tuple → tuple[type1, type2, ...] 3. Replace dict → dict[key_type, val_type] 4. Replace list → list[element_type] 5. Run mypy --strict to validate","expected_gain":"Better IDE support, catch type bugs early, improved documentation","references":["https://mypy.readthedocs.io/en/stable/cheat_sheet_py3.html"]}
{"rank":16,"category":"security","subcategory":"sanitization","issue":"Weak sensitive keyword detection (easily bypassed)","impact":"LOW","effort":"LOW","file":"clockify_support_cli_final.py","line":1940,"current":"looks_sensitive() uses simple substring matching (e.g., 'password' in query)","proposed":"Use regex with common character substitutions (p[a@]ssw[o0]rd, tok[e3]n, etc.)","rationale":"Simple substring matching is easily bypassed with character substitutions (p@ssword, pa$$word). Regex patterns catch common obfuscations and improve detection.","implementation":"1. Define regex patterns for sensitive terms with substitutions 2. Compile patterns once at module load 3. Use re.search(pattern, query.lower()) for matching 4. Add test cases for obfuscation attempts","expected_gain":"Better detection of sensitive queries, improved policy enforcement","references":[]}
{"rank":17,"category":"correctness","subcategory":"normalization","issue":"Score normalization returns zeros when std=0, losing rank information","impact":"LOW","effort":"LOW","file":"clockify_support_cli_final.py","line":1290,"current":"normalize_scores_zscore() returns zeros when std=0 (all scores identical)","proposed":"Return original array when std=0 (preserve relative order even if all equal)","rationale":"When all scores are identical (e.g., all zeros), returning zeros is correct. But when all scores are non-zero (e.g., all 0.5), returning zeros loses information. Returning original preserves any tie-breaking logic downstream.","implementation":"Replace 'return np.zeros_like(a)' with 'return a' when s == 0","expected_gain":"Preserve rank order in edge cases, prevent unexpected tie-breaking behavior","references":[]}
{"rank":18,"category":"developer_experience","subcategory":"makefile","issue":"Makefile doesn't auto-activate venv (requires manual source)","impact":"LOW","effort":"LOW","file":"Makefile","line":32,"current":"Targets assume venv is activated, user must manually run 'source rag_env/bin/activate'","proposed":"Prefix all commands with 'source rag_env/bin/activate &&' or use venv/bin/python directly","rationale":"Manual venv activation is error-prone (easy to forget). Auto-activation in Makefile improves DX and prevents 'command not found' errors.","implementation":"Option 1: Prefix commands with 'source rag_env/bin/activate &&' Option 2: Use '$(shell pwd)/rag_env/bin/python' directly Option 3: Add SHELL := /bin/bash and .ONESHELL: directive","expected_gain":"Better developer experience, fewer activation errors, one-command workflows","references":[]}
{"rank":19,"category":"performance","subcategory":"query_expansion","issue":"Query expansion adds all synonyms without relevance weighting","impact":"LOW","effort":"LOW","file":"clockify_support_cli_final.py","line":1341,"current":"expand_query() appends all synonyms for matched terms (no weighting)","proposed":"Limit to top-3 most relevant synonyms, weight by usage frequency in KB","rationale":"Adding all synonyms (e.g., 4 synonyms for 'track' + 3 for 'time') dilutes signal and increases noise. Top-3 synonyms weighted by KB frequency improves precision.","implementation":"1. Precompute synonym frequencies in KB during build 2. Sort synonyms by frequency 3. Limit to top-3 per term 4. Weight by frequency (optional: multiply BM25 scores by weight)","expected_gain":"2-5% improvement in P@5 by reducing noise from irrelevant synonyms","references":[]}
{"rank":20,"category":"testing","subcategory":"ann","issue":"Missing tests for FAISS/ANN index building and querying","impact":"LOW","effort":"LOW","file":"tests/","line":null,"current":"No tests for FAISS index creation, loading, or querying","proposed":"Add tests/test_faiss.py to verify index correctness and performance","rationale":"FAISS is critical for performance (10-50x speedup) but untested. Index corruption or query errors would go undetected.","implementation":"1. Create tests/test_faiss.py 2. Test build_faiss_index() returns valid index 3. Test save/load roundtrip 4. Test search returns correct top-k 5. Test M1 fallback to FlatIP","expected_gain":"Catch FAISS bugs early, validate M1 compatibility, ensure consistent behavior","references":[]}
{"rank":21,"category":"RAG","subcategory":"prompting","issue":"Static few-shot examples in system prompt (not query-adaptive)","impact":"MEDIUM","effort":"MEDIUM","file":"clockify_support_cli_final.py","line":697,"current":"SYSTEM_PROMPT has 3 hardcoded few-shot examples (same for all queries)","proposed":"Dynamically select k=3 most similar examples from example pool based on query embedding","rationale":"Query-adaptive few-shot examples improve answer formatting and citation consistency by 5-10%. Similar examples provide better guidance for LLM.","implementation":"1. Create example pool (50-100 examples from ground truth) 2. Embed all examples once 3. At query time, retrieve k=3 most similar examples 4. Format into few-shot prompt 5. Prepend to user message","expected_gain":"5-10% improvement in answer formatting consistency, better citation accuracy","references":["https://arxiv.org/abs/2005.14165"]}
{"rank":22,"category":"performance","subcategory":"memory","issue":"No mmap mode for large embedding arrays (memory inefficiency)","impact":"LOW","effort":"LOW","file":"clockify_support_cli_final.py","line":1869,"current":"Code has mmap_mode='r' but not consistently used everywhere","proposed":"Ensure mmap_mode='r' used consistently for all embedding loads, document memory benefits","rationale":"mmap mode reduces memory footprint by 50-80% for large KBs (10k+ chunks, 30+ MB embeddings). Already implemented in one place but not documented or consistently used.","implementation":"1. Audit all np.load(FILES['emb']) calls 2. Add mmap_mode='r' where missing 3. Document memory savings in CLAUDE.md 4. Add memory benchmarks to benchmark.py","expected_gain":"50-80% memory reduction for large KBs, enables larger deployments on smaller machines","references":["https://numpy.org/doc/stable/reference/generated/numpy.memmap.html"]}
{"rank":23,"category":"RAG","subcategory":"chunking","issue":"Metadata-poor chunks (only title, URL, section - no provenance)","impact":"MEDIUM","effort":"LOW","file":"chunking.py","line":168,"current":"Chunks store id, title, url, section only (no parent doc, position, neighbors)","proposed":"Add parent_doc_id, chunk_position, prev_chunk_id, next_chunk_id for richer context","rationale":"Metadata-rich chunks enable better context for LLM (can reference neighbors), improved citation accuracy (cite parent doc), and better debugging (trace chunk provenance).","implementation":"1. During build_chunks(), assign parent_doc_id = article_id 2. Track chunk_position = idx in article 3. Store prev_chunk_id = chunks[idx-1]['id'] if idx > 0 4. Post-process to fill next_chunk_id 5. Include in snippet formatting for LLM","expected_gain":"Better LLM context, improved citation accuracy, easier debugging","references":[]}
{"rank":24,"category":"RAG","subcategory":"retrieval","issue":"No multi-hop query support (decomposition for complex questions)","impact":"MEDIUM","effort":"HIGH","file":"clockify_support_cli_final.py","line":2373,"current":"Single-turn retrieval only, complex questions processed as-is","proposed":"Add query decomposition to break complex questions into sub-queries, retrieve independently","rationale":"Complex multi-part questions ('What are pricing tiers and which features are in each?') need decomposition. Retrieving for sub-queries improves recall and answer completeness.","implementation":"1. Add decompose_query() using LLM or rule-based heuristics 2. Detect multi-part questions (conjunctions, multiple '?') 3. Break into 2-3 sub-queries 4. Retrieve for each independently 5. Merge results with deduplication 6. Pass combined context to LLM","expected_gain":"10-20% improvement on complex questions, better answer completeness","references":["https://arxiv.org/abs/2212.10509"]}
{"rank":25,"category":"performance","subcategory":"http","issue":"No connection keep-alive metrics or monitoring","impact":"LOW","effort":"LOW","file":"http_utils.py","line":59,"current":"Connection pooling enabled but no metrics on pool usage, connection reuse","proposed":"Add metrics for connection pool stats (active, idle, created, closed)","rationale":"Connection pool metrics help diagnose performance issues (too many connections, pool exhaustion). Prometheus counters enable monitoring and alerting.","implementation":"1. Add prometheus_client dependency (optional) 2. Instrument get_session() with pool metrics 3. Export pool_connections_active, pool_connections_idle, etc. 4. Add /metrics endpoint or log to structured logs","expected_gain":"Better visibility into HTTP performance, diagnose connection issues faster","references":["https://prometheus.io/docs/instrumenting/clientlibs/"]}
{"rank":26,"category":"RAG","subcategory":"evaluation","issue":"No automated metric tracking in production logs","impact":"MEDIUM","effort":"LOW","file":"caching.py","line":186,"current":"log_query() logs queries but doesn't compute/log retrieval metrics","proposed":"Compute and log MRR, NDCG, P@K for queries where ground truth is available","rationale":"Tracking metrics in production enables continuous monitoring of retrieval quality, detection of regressions, and A/B testing. Critical for production ML systems.","implementation":"1. Load ground truth cache at startup (if available) 2. In log_query(), check if question_hash in ground_truth 3. If yes, compute MRR, NDCG@10, P@5 4. Add to log_entry['metrics'] = {...} 5. Aggregate daily for dashboards","expected_gain":"Continuous quality monitoring, early regression detection, enable A/B testing","references":["https://arxiv.org/abs/2104.08663"]}
{"rank":27,"category":"code_quality","subcategory":"constants","issue":"Magic numbers throughout code (200, 500, 0.5, 0.7, etc.)","impact":"LOW","effort":"LOW","file":"*.py","line":null,"current":"Hardcoded constants without names (e.g., 200 FAISS candidates, 500 rerank snippet length)","proposed":"Extract all magic numbers to config.py with descriptive names and documentation","rationale":"Magic numbers hurt readability and maintainability. Named constants with docs make code self-documenting and easier to tune.","implementation":"1. Audit code for magic numbers 2. Add to config.py: FAISS_CANDIDATE_MULTIPLIER = 3, RERANK_SNIPPET_MAX_CHARS = 500, etc. 3. Replace all occurrences 4. Document rationale in comments","expected_gain":"Better code readability, easier parameter tuning, self-documenting code","references":[]}
{"rank":28,"category":"developer_experience","subcategory":"setup","issue":"No single-command dev setup (requires multiple steps)","impact":"LOW","effort":"LOW","file":"Makefile","line":null,"current":"Setup requires: make venv → source activate → make install → make build","proposed":"Add 'make dev' target: creates venv, installs deps, pre-commit hooks, builds index","rationale":"One-command setup improves onboarding experience, reduces setup errors, saves time.","implementation":"Add to Makefile: dev: venv install pre-commit-install build @echo '✅ Dev environment ready. Run: make chat'","expected_gain":"Faster onboarding, better developer experience, fewer setup issues","references":[]}
{"rank":29,"category":"testing","subcategory":"fixtures","issue":"Test fixtures not shared (each test file duplicates setup)","impact":"LOW","effort":"LOW","file":"tests/","line":null,"current":"Each test file creates own fixtures (chunks, embeddings, BM25)","proposed":"Create tests/conftest.py with shared fixtures (sample_chunks, sample_embeddings, sample_bm25)","rationale":"Shared fixtures reduce duplication, ensure consistent test data, speed up test development.","implementation":"1. Create tests/conftest.py 2. Define @pytest.fixture: sample_chunks, sample_embeddings, sample_bm25 3. Remove duplicates from test files 4. Reference fixtures by name in test functions","expected_gain":"Reduced test code duplication, consistent test data, faster test development","references":["https://docs.pytest.org/en/stable/how-to/fixtures.html#scope-sharing-fixtures-across-classes-modules-packages-or-session"]}
{"rank":30,"category":"RAG","subcategory":"evaluation","issue":"No adversarial test cases (robustness testing)","impact":"LOW","effort":"MEDIUM","file":"tests/","line":null,"current":"Tests cover happy path only, no adversarial cases (empty query, extremely long query, injection attempts)","proposed":"Add adversarial test suite: empty/whitespace queries, >2000 char queries, prompt injection attempts, unicode edge cases","rationale":"Adversarial tests catch edge cases and security issues. Important for production robustness.","implementation":"1. Create tests/test_adversarial.py 2. Test empty/whitespace queries → expect ValueError 3. Test >2000 char queries → expect ValueError 4. Test prompt injection patterns → expect sanitization 5. Test unicode edge cases (emoji, RTL, zero-width)","expected_gain":"Improved robustness, catch security issues, better error handling","references":["https://owasp.org/www-project-machine-learning-security-top-10/"]}
