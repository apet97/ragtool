{"rank": 1, "category": "correctness", "subcategory": "evaluation", "issue": "Evaluation script cannot run because fallback import is outside the try block", "impact": "HIGH", "effort": "LOW", "file": "eval.py", "line": 260, "current": "    if not rag_available:\n        ...\n        from clockify_support_cli_final import load_index, retrieve\n    except ImportError as e:\n        print(f\"Error importing RAG functions: {e}\")", "proposed": "Keep the fallback import inside the lexical branch's try/except and add a dedicated ImportError handler around the hybrid import.", "rationale": "The dangling except triggers a syntax/runtime error, so `python eval.py` exits before running metrics.", "implementation": "Indent the fallback import under the preceding try block and catch ImportError where `load_index` is first imported.", "expected_gain": "Restores automated retrieval quality checks and CI gating.", "references": []}
{"rank": 2, "category": "performance", "subcategory": "retrieval", "issue": "FAISS path recomputes dense scores for the entire corpus", "impact": "HIGH", "effort": "MEDIUM", "file": "clockify_support_cli_final.py", "line": 1567, "current": "        dense_scores_full = np.empty(n_chunks, dtype=np.float32)\n        ...\n            dense_scores_full[remaining_idx] = vecs_n[remaining_idx].dot(qv_n)", "proposed": "Only score FAISS candidates and lazily materialize other indices instead of dotting the whole matrix.", "rationale": "Recomputing scores for every chunk erases the ANN speedup and keeps latency O(n).", "implementation": "Return candidate scores directly, store them in DenseScoreStore with lazy callbacks, and avoid allocating `dense_scores_full` for all rows.", "expected_gain": "2-3x faster queries on large corpora with FAISS enabled.", "references": []}
{"rank": 3, "category": "correctness", "subcategory": "caching", "issue": "Query cache key ignores retrieval parameters", "impact": "HIGH", "effort": "LOW", "file": "clockify_support_cli_final.py", "line": 2693, "current": "    cached_result = QUERY_CACHE.get(question)", "proposed": "Include `top_k`, `pack_top`, `use_rerank`, and threshold in the cache key or skip caching when options differ.", "rationale": "Reusing an answer generated with different settings leads to wrong citations/snippets.", "implementation": "Compute a composite key (hash of question + settings) and pass it through QueryCache APIs.", "expected_gain": "Prevents stale or misleading responses when operators adjust retrieval knobs.", "references": []}
{"rank": 4, "category": "RAG", "subcategory": "retrieval", "issue": "Query expansion order is nondeterministic", "impact": "MEDIUM", "effort": "LOW", "file": "clockify_support_cli_final.py", "line": 1430, "current": "    expanded_terms = set()", "proposed": "Accumulate synonyms in insertion order (list or OrderedDict) and sort before joining.", "rationale": "Set iteration order varies across runs, producing different expanded queries, hurting reproducibility and caching.", "implementation": "Switch to a list, guard against duplicates, and join with deterministic ordering.", "expected_gain": "Consistent retrieval results and stable logs/cache entries.", "references": []}
{"rank": 5, "category": "RAG", "subcategory": "context", "issue": "Snippet packer estimates tokens using chars/4", "impact": "HIGH", "effort": "MEDIUM", "file": "clockify_support_cli_final.py", "line": 1796, "current": "        hdr_tokens = approx_tokens(len(hdr) + 1)", "proposed": "Use an actual tokenizer (e.g., tiktoken) or cached token counts from embedding step to enforce the context budget accurately.", "rationale": "Character heuristics undercount multilingual text, allowing over-budget prompts or over-truncation.", "implementation": "Add a tokenizer utility and replace `approx_tokens` calls with exact token counts, caching results per chunk.", "expected_gain": "Reliable context packing and fewer LLM truncation failures.", "references": []}
{"rank": 6, "category": "performance", "subcategory": "logging", "issue": "`retrieved_chunks` is built twice on success path", "impact": "MEDIUM", "effort": "LOW", "file": "clockify_support_cli_final.py", "line": 2865, "current": "        retrieved_chunks = []\n        for pack_rank, chunk_id in enumerate(ids):", "proposed": "Consolidate into a single loop that captures both pack metadata and dense scores.", "rationale": "The first loop’s work is discarded, wasting CPU and obscuring intended log structure.", "implementation": "Merge the data needed for query logging into one loop and reuse the same list downstream.", "expected_gain": "Eliminates redundant per-query allocations (especially with high top_k).", "references": []}
{"rank": 7, "category": "performance", "subcategory": "logging", "issue": "Log formatter normalizes chunks before checking logging toggle", "impact": "MEDIUM", "effort": "LOW", "file": "clockify_support_cli_final.py", "line": 2434, "current": "    normalized_chunks = []", "proposed": "Early-return when logging is disabled before cloning chunk dictionaries.", "rationale": "When logging is off (common in production), the current code still copies every chunk, adding latency.", "implementation": "Move the `QUERY_LOG_DISABLED` guard to the top and only build `normalized_chunks` afterwards.", "expected_gain": "Avoids needless work on every query when telemetry is disabled.", "references": []}
{"rank": 8, "category": "correctness", "subcategory": "retrieval", "issue": "Coverage gate is evaluated twice and rebuilds index map", "impact": "MEDIUM", "effort": "LOW", "file": "clockify_support_cli_final.py", "line": 2742, "current": "        coverage_pass = coverage_ok(mmr_selected, dense_scores_all, threshold)", "proposed": "Compute coverage once using the existing DenseScoreStore and build the `chunk_id_to_index` map only after the gate succeeds.", "rationale": "Duplicated checks add noise and rebuild the same map even when coverage fails.", "implementation": "Keep the first call, drop the duplicate, and move `chunk_id_to_index` construction after the coverage guard.", "expected_gain": "Slightly faster refusals and clearer control flow.", "references": []}
{"rank": 9, "category": "RAG", "subcategory": "answer_quality", "issue": "LLM answer JSON is trusted without citation validation", "impact": "HIGH", "effort": "MEDIUM", "file": "clockify_support_cli_final.py", "line": 2608, "current": "        if isinstance(parsed, dict):", "proposed": "Inspect the parsed JSON for a citations field or extract bracketed references and cross-check against packed chunk IDs.", "rationale": "Without validation, the assistant can output fabricated citations despite the guardrails.", "implementation": "Parse citation tokens from `answer`, compare to `ids`, and fall back to refusal or rewrite citations on mismatch.", "expected_gain": "Reduces hallucinated or invalid citations in final answers.", "references": []}
{"rank": 10, "category": "performance", "subcategory": "indexing", "issue": "Ollama embedding calls run strictly sequentially", "impact": "HIGH", "effort": "HIGH", "file": "clockify_rag/embedding.py", "line": 74, "current": "    for i, t in enumerate(texts):", "proposed": "Batch prompts when the API supports arrays or introduce worker threads respecting the rate limiter.", "rationale": "Large builds spend most time waiting on sequential HTTP requests.", "implementation": "Add a batching helper (chunk size 8-16) or parallelize with a thread pool bounded by `RateLimiter`.", "expected_gain": "Cuts knowledge base build times by 3-5x on large corpora.", "references": []}
{"rank": 11, "category": "performance", "subcategory": "indexing", "issue": "FAISS training sampling is non-deterministic", "impact": "MEDIUM", "effort": "LOW", "file": "clockify_rag/indexing.py", "line": 90, "current": "        train_indices = np.random.choice(len(vecs), train_size, replace=False)", "proposed": "Seed the RNG (e.g., with config DEFAULT_SEED) or accept an RNG argument so builds are reproducible.", "rationale": "Unseeded sampling yields different IVF centroids between builds, complicating diffing and debugging.", "implementation": "Inject `np.random.default_rng(seed)` and use it for sampling, or accept an RNG from the caller.", "expected_gain": "Reproducible ANN indexes across rebuilds.", "references": []}
{"rank": 12, "category": "security", "subcategory": "logging", "issue": "Query logs always persist full chunk text", "impact": "MEDIUM", "effort": "LOW", "file": "clockify_support_cli_final.py", "line": 2895, "current": "                    \"chunk\": chunk", "proposed": "Honor logging flags and omit or redact chunk text unless explicitly enabled.", "rationale": "Storing raw documentation in logs may violate data-handling policies and is unnecessary for metrics.", "implementation": "Include only chunk IDs/scores by default, adding body text only when the env flag is true.", "expected_gain": "Safer production telemetry and smaller log files.", "references": []}
{"rank": 13, "category": "security", "subcategory": "network", "issue": "DeepSeek shim exposes API without throttling", "impact": "MEDIUM", "effort": "MEDIUM", "file": "deepseek_ollama_shim.py", "line": 104, "current": "    def _check_access(self):", "proposed": "Add a simple token-bucket rate limiter and request logging to mitigate abuse.", "rationale": "Anyone with the token can spam the shim, exhausting API quota or causing DoS.", "implementation": "Introduce a global RateLimiter (reuse caching.RateLimiter) and log request metadata for auditing.", "expected_gain": "Improved resilience and observability when exposing the shim.", "references": []}
{"rank": 14, "category": "performance", "subcategory": "retrieval", "issue": "Linear fallback recomputes dot product twice", "impact": "MEDIUM", "effort": "LOW", "file": "clockify_support_cli_final.py", "line": 1592, "current": "        dense_scores = vecs_n.dot(qv_n)", "proposed": "Reuse `dense_scores_full` instead of invoking `vecs_n.dot` a second time and derive candidate indices from its shape.", "rationale": "The second dot product doubles compute in the common linear path.", "implementation": "Assign `dense_scores = dense_scores_full` and reuse it downstream, removing the redundant call.", "expected_gain": "~2x faster retrieval when ANN is disabled.", "references": []}
{"rank": 15, "category": "correctness", "subcategory": "logging", "issue": "`LOG_QUERY_INCLUDE_ANSWER` flag is never honored", "impact": "MEDIUM", "effort": "LOW", "file": "clockify_support_cli_final.py", "line": 2470, "current": "    log_entry = {", "proposed": "Add conditional logic to redact or replace the answer payload based on config flags.", "rationale": "Operators expect the documented toggle to remove sensitive answers from logs.", "implementation": "When the flag is false, drop `answer` entirely or replace with `LOG_QUERY_ANSWER_PLACEHOLDER` before writing the JSON line.", "expected_gain": "Aligns runtime behavior with documented privacy controls.", "references": []}
{"rank": 16, "category": "performance", "subcategory": "retrieval", "issue": "Benchmark labeled BM25 actually measures hybrid retrieval", "impact": "LOW", "effort": "LOW", "file": "benchmark.py", "line": 240, "current": "def benchmark_retrieval_bm25(...):", "proposed": "Either call the BM25-only scorer or rename the benchmark to reflect hybrid retrieval.", "rationale": "Misleading labels hide regressions and confuse dashboard consumers.", "implementation": "Expose a BM25-only helper or update the function/metric names.", "expected_gain": "Accurate performance reporting and easier regression tracking.", "references": []}
{"rank": 17, "category": "documentation", "subcategory": "status", "issue": "README still claims \"Production-Ready\" despite blocking defects", "impact": "MEDIUM", "effort": "LOW", "file": "README.md", "line": 3, "current": "**Status**: ✅ Production-Ready", "proposed": "Revise the status badge and call out known blockers until fixed.", "rationale": "Inaccurate status misleads operators and stakeholders about current readiness.", "implementation": "Update the headline to reflect audit findings and add a short `Pending Fixes` section.", "expected_gain": "Sets correct expectations for deployers and auditors.", "references": []}
{"rank": 18, "category": "correctness", "subcategory": "answer_quality", "issue": "LLM parsing fallback treats raw text as valid answer", "impact": "MEDIUM", "effort": "MEDIUM", "file": "clockify_support_cli_final.py", "line": 2614, "current": "    answer = raw_response  # Default to raw response if parsing fails", "proposed": "If JSON parsing fails repeatedly, return a refusal or retry with a simpler prompt instead of accepting raw text without citations.", "rationale": "Raw responses often omit mandated citation format, increasing hallucination risk.", "implementation": "Track parse failures, trigger a secondary prompt without JSON, or emit the refusal string to maintain policy compliance.", "expected_gain": "Reduces citation-less or policy-violating answers when the model emits plain text.", "references": []}
{"rank": 19, "category": "security", "subcategory": "logging", "issue": "Custom log paths are not created automatically", "impact": "LOW", "effort": "LOW", "file": "clockify_support_cli_final.py", "line": 2489, "current": "    try:\n        with open(QUERY_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n            f.write(json.dumps(log_entry) + \"\\n\")", "proposed": "Ensure parent directories exist and surface errors clearly instead of silently warning.", "rationale": "Pointing logs at `/var/log/...` currently fails unless directories pre-exist.", "implementation": "Add `os.makedirs(Path(QUERY_LOG_FILE).parent, exist_ok=True)` before opening and re-raise when logging is enabled.", "expected_gain": "Reliable logging setup and fewer silent telemetry gaps.", "references": []}
{"rank": 20, "category": "performance", "subcategory": "retrieval", "issue": "ANN fallback always expands candidate list to full corpus", "impact": "LOW", "effort": "LOW", "file": "clockify_support_cli_final.py", "line": 1600, "current": "    if not candidate_idx:\n        dense_scores_full = vecs_n.dot(qv_n)\n        candidate_idx = np.arange(len(chunks)).tolist()", "proposed": "Limit fallback candidate generation to the requested `top_k * FAISS_CANDIDATE_MULTIPLIER` instead of force-loading every index.", "rationale": "When FAISS returns zero hits we revert to full linear scan even if the corpus is large.", "implementation": "Sample the top scores from `dense_scores_full` without materializing every index and reuse existing ranking logic.", "expected_gain": "Maintains responsiveness on sparse corpora or cold-start scenarios.", "references": []}
